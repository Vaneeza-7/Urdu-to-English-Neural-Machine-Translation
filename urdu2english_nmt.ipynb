{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGC83Hubz0Fg"
      },
      "outputs": [],
      "source": [
        "def load_file(filepath):\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "    return [sentence.strip() for sentence in sentences]\n",
        "\n",
        "# Load all files\n",
        "eng_dev = load_file('/content/eng_Latn.dev')\n",
        "eng_devtest = load_file('/content/eng_Latn.devtest')\n",
        "urdu_dev = load_file('/content/urd_Arab.dev')\n",
        "urdu_devtest = load_file('/content/urd_Arab.devtest')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing and Tensor Creation:\n",
        "Following steps were followed:\n",
        "- Concatenation of .dev and .devtest files for both urdu and english.\n",
        "- Preprocessing: preserve abbreviations, remove extra spaces, and add spaces around punctuation marks.\n",
        "- Tokenization\n",
        "- Vocabulary creation\n",
        "- Converting sentences (tokenized) to tensors\n",
        "- Shuffling and splitting into validation, test, and training sets."
      ],
      "metadata": {
        "id": "oPgLApItIAP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine sentences from dev and devtest for English and Urdu\n",
        "english_sentences = eng_dev + eng_devtest\n",
        "urdu_sentences = urdu_dev + urdu_devtest\n",
        "\n",
        "assert len(english_sentences) == len(urdu_sentences), \"Mismatch in sentence alignment!\"\n",
        "\n",
        "print(f\"Total aligned sentences: {len(english_sentences)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo9bgr7a1hci",
        "outputId": "e9f47387-d58a-44b0-8534-af7739103560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total aligned sentences: 2009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess_text(sentence, lang=\"urdu\"):\n",
        "    if lang == \"english\":\n",
        "        # Preserve abbreviations like U.S., U.K., etc.\n",
        "        abbreviation_pattern = r'\\b([A-Za-z])\\.([A-Za-z])\\.'\n",
        "        sentence = re.sub(abbreviation_pattern, r'\\1.\\2.', sentence)\n",
        "\n",
        "        # Add space around other punctuation\n",
        "        sentence = re.sub(r\"([!?])\", r\" \\1 \", sentence)  # Exclude periods in abbreviations\n",
        "        sentence = re.sub(r\"(?<!\\w)\\.(?!\\w)\", r\" . \", sentence)\n",
        "        # convert to lowercase\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "    elif lang == \"urdu\":\n",
        "        # Add space around Urdu full stop\n",
        "        sentence = re.sub(r\"۔\", \" ۔ \", sentence)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
        "    return sentence\n",
        "\n",
        "english_sentences = [preprocess_text(sentence, lang=\"english\") for sentence in english_sentences]\n",
        "urdu_sentences = [preprocess_text(sentence, lang=\"urdu\") for sentence in urdu_sentences]"
      ],
      "metadata": {
        "id": "z5sAaVE755Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "english_tokenized = [nltk.word_tokenize(sentence) for sentence in english_sentences]\n",
        "urdu_tokenized = [nltk.word_tokenize(sentence) for sentence in urdu_sentences]\n",
        "\n",
        "print(f\"Tokenized English sentences: {english_tokenized[:5]}\")\n",
        "print(f\"Tokenized Urdu sentences: {urdu_tokenized[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-WzMBuU1wAj",
        "outputId": "fbdb2dbd-ba28-4731-e112-2a0c4e17a321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized English sentences: [['on', 'monday', ',', 'scientists', 'from', 'the', 'stanford', 'university', 'school', 'of', 'medicine', 'announced', 'the', 'invention', 'of', 'a', 'new', 'diagnostic', 'tool', 'that', 'can', 'sort', 'cells', 'by', 'type', ':', 'a', 'tiny', 'printable', 'chip', 'that', 'can', 'be', 'manufactured', 'using', 'standard', 'inkjet', 'printers', 'for', 'possibly', 'about', 'one', 'u.s.', 'cent', 'each', '.'], ['lead', 'researchers', 'say', 'this', 'may', 'bring', 'early', 'detection', 'of', 'cancer', ',', 'tuberculosis', ',', 'hiv', 'and', 'malaria', 'to', 'patients', 'in', 'low-income', 'countries', ',', 'where', 'the', 'survival', 'rates', 'for', 'illnesses', 'such', 'as', 'breast', 'cancer', 'can', 'be', 'half', 'those', 'of', 'richer', 'countries', '.'], ['the', 'jas', '39c', 'gripen', 'crashed', 'onto', 'a', 'runway', 'at', 'around', '9:30', 'am', 'local', 'time', '(', '0230', 'utc', ')', 'and', 'exploded', ',', 'closing', 'the', 'airport', 'to', 'commercial', 'flights', '.'], ['the', 'pilot', 'was', 'identified', 'as', 'squadron', 'leader', 'dilokrit', 'pattavee', '.'], ['local', 'media', 'reports', 'an', 'airport', 'fire', 'vehicle', 'rolled', 'over', 'while', 'responding', '.']]\n",
            "Tokenized Urdu sentences: [['پیر', 'کے', 'روز،', 'سٹینفورڈ', 'اسکول', 'آف', 'میڈیسن', 'کے', 'سائنسدانوں', 'نے', 'ایک', 'جدید', 'تشخیصی', 'آلہ', 'دریافت', 'کرنے', 'کا', 'اعلان', 'کیا', 'ہے', 'جو', 'خلیوں', 'کو', 'اس', 'کی', 'اقسام', 'کے', 'لحاظ', 'سے', 'ترتیب', 'دے', 'سکتا', 'ہے', ':', 'یہ', 'ایک', 'چھوٹی', 'سی', 'پرنٹیبل', 'چپ', 'ہے', 'جو', 'غالباً', 'ایک', 'امریکی', 'سنٹ', 'میں', 'معیاری', 'انک', 'جیٹ', 'پرنٹرز', 'کا', 'استعمال', 'کر', 'کے', 'تیار', 'کی', 'جا', 'سکتی', 'ہے-'], ['سرکردہ', 'محققین', 'کہتے', 'ہیں', 'کہ', 'اس', 'سے', 'کم', 'آمدنی', 'والے', 'ممالک', 'کے', 'مریضوں', 'میں', 'کینسر،', 'تپ', 'و', 'دق،', 'ایچ', 'آئی', 'وی', 'اور', 'ملیریا', 'کا', 'جلد', 'پتہ', 'چل', 'سکتا', 'ہے،', 'جہاں', 'چھاتی', 'کے', 'کینسر', 'جیسی', 'بیماریوں', 'سے', 'بچنے', 'کی', 'شرح', 'امیر', 'ممالک', 'کی', 'مقابلے', 'میں', 'نصف', 'ہو', 'سکتی', 'ہے', '۔'], ['مقامی', 'وقت', 'کے', 'مطابق', 'تقریبا', 'صبح', '9:30', 'بجے', '(', '0230', 'UTC', ')', '39C', 'JASگریپین', 'رن', 'وے', 'پر', 'ٹکرا', 'کر', 'دھماکے', 'کے', 'ساتھ', 'پھٹ', 'کر', 'تباہ', 'ہو', 'گیا،', 'جس', 'کے', 'باعث', 'ہوائی', 'اڈے', 'کی', 'تجارتی', 'پروازیں', 'بند', 'کرنی', 'پڑیں-'], ['پائلٹ', 'کو', 'اسکواڈرن', 'لیڈر', 'ڈیلوکرٹ', 'پٹاوی', 'کے', 'نام', 'سے', 'شناخت', 'کیا', 'گیا', '۔'], ['مقامی', 'میڈیا', 'نے', 'بتایا', 'کہ', 'ہوائی', 'اڈے', 'کی', 'ایک', 'آگ', 'بھجانے', 'والی', 'گاڑی', 'کاروائی', 'کرتے', 'وقت', 'الٹ', 'گئی', '۔']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "X7pqYLL8R-vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Word2Vec for English\n",
        "english_w2v_model = Word2Vec(sentences=english_tokenized, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Word2Vec for Urdu\n",
        "urdu_w2v_model = Word2Vec(sentences=urdu_tokenized, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "english_w2v_model.save(\"english_word2vec.model\")\n",
        "urdu_w2v_model.save(\"urdu_word2vec.model\")"
      ],
      "metadata": {
        "id": "f1ePgh27R9u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def build_vocab_with_embeddings(tokenized_sentences, w2v_model, embedding_dim):\n",
        "    vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "    index = len(vocab)\n",
        "    embeddings = [np.zeros(embedding_dim),  # For <PAD>\n",
        "                  np.random.uniform(-0.1, 0.1, embedding_dim),  # For <SOS>\n",
        "                  np.random.uniform(-0.1, 0.1, embedding_dim),  # For <EOS>\n",
        "                  np.random.uniform(-0.1, 0.1, embedding_dim)]  # For <UNK>\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        for word in sentence:\n",
        "            if word not in vocab:\n",
        "                vocab[word] = index\n",
        "                if word in w2v_model.wv:\n",
        "                    embeddings.append(w2v_model.wv[word])\n",
        "                else:\n",
        "                    embeddings.append(np.random.uniform(-0.1, 0.1, embedding_dim))  # For OOV words\n",
        "                index += 1\n",
        "\n",
        "    return vocab, np.array(embeddings)\n",
        "\n",
        "english_vocab, english_embeddings = build_vocab_with_embeddings(english_tokenized, english_w2v_model, 300)\n",
        "urdu_vocab, urdu_embeddings = build_vocab_with_embeddings(urdu_tokenized, urdu_w2v_model, 300)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NoRHnJuySTdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"English vocab size: {len(english_vocab)}\")\n",
        "print(f\"Urdu vocab size: {len(urdu_vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buNwxDnZT88X",
        "outputId": "0b629cab-779b-441c-d721-11c7c5bf8fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocab size: 8651\n",
            "Urdu vocab size: 9555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def build_vocab(tokenized_sentences):\n",
        "#     vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "#     index = 4\n",
        "#     for sentence in tokenized_sentences:\n",
        "#         for word in sentence:\n",
        "#             if word not in vocab:\n",
        "#                 vocab[word] = index\n",
        "#                 index += 1\n",
        "#     return vocab\n",
        "\n",
        "# english_vocab = build_vocab(english_tokenized)\n",
        "# urdu_vocab = build_vocab(urdu_tokenized)\n",
        "\n",
        "# print(\"English vocab size:\", len(english_vocab))\n",
        "# print(\"Urdu vocab size:\", len(urdu_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHH9zA732Y6D",
        "outputId": "d2778399-411c-4a88-b8be-03a2ddda815b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocab size: 8651\n",
            "Urdu vocab size: 9555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert '<PAD>' in english_vocab and '<PAD>' in urdu_vocab, \"Missing <PAD> in vocab!\"\n",
        "assert '<SOS>' in english_vocab and '<SOS>' in urdu_vocab, \"Missing <SOS> in vocab!\"\n",
        "assert '<EOS>' in english_vocab and '<EOS>' in urdu_vocab, \"Missing <EOS> in vocab!\"\n",
        "assert '<UNK>' in english_vocab and '<UNK>' in urdu_vocab, \"Missing <UNK> in vocab!\"\n",
        "\n",
        "print(\"English and Urdu vocabularies contain required tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyzF1pxzvSEN",
        "outputId": "a58d7656-6ab9-41c1-cfad-d84215abe94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English and Urdu vocabularies contain required tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#convert sentences to tensors\n",
        "\n",
        "def sentences_to_tensor(tokenized_sentences, vocab, max_length=50):\n",
        "    tensors = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        tokens = [vocab.get(word, vocab['<UNK>']) for word in sentence]\n",
        "        # Add <SOS> and <EOS>\n",
        "        tokens = [vocab['<SOS>']] + tokens[:max_length-2] + [vocab['<EOS>']]\n",
        "        # Pad to max_length\n",
        "        tokens += [vocab['<PAD>']] * (max_length - len(tokens))\n",
        "        tensors.append(tokens)\n",
        "    return torch.tensor(tensors, dtype=torch.long)\n",
        "\n",
        "max_length = 50\n",
        "english_tensors = sentences_to_tensor(english_tokenized, english_vocab, max_length)\n",
        "urdu_tensors = sentences_to_tensor(urdu_tokenized, urdu_vocab, max_length)\n",
        "\n",
        "print(\"English tensor shape:\", english_tensors.shape)\n",
        "print(\"Urdu tensor shape:\", urdu_tensors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipVAyxpD87nt",
        "outputId": "43d48edb-6e73-4036-ba01-73f04f109100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English tensor shape: torch.Size([2009, 50])\n",
            "Urdu tensor shape: torch.Size([2009, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.max(english_tensors) < len(english_vocab), \"English tensors contain out-of-range indices!\"\n",
        "assert torch.max(urdu_tensors) < len(urdu_vocab), \"Urdu tensors contain out-of-range indices!\"\n",
        "assert torch.min(english_tensors) >= 0, \"English tensors contain negative indices!\"\n",
        "assert torch.min(urdu_tensors) >= 0, \"Urdu tensors contain negative indices!\"\n",
        "\n",
        "print(\"English and Urdu tensors contain valid indices.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbE4N1r8vZX9",
        "outputId": "b812db2e-3643-47d6-d473-97d5e2ae9ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English and Urdu tensors contain valid indices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_eng, test_eng, train_urdu, test_urdu = train_test_split(\n",
        "    english_tensors, urdu_tensors, test_size=0.3, random_state=42\n",
        ")\n",
        "val_eng, test_eng, val_urdu, test_urdu = train_test_split(\n",
        "    test_eng, test_urdu, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(train_eng)} samples\")\n",
        "print(f\"Validation set: {len(val_eng)} samples\")\n",
        "print(f\"Test set: {len(test_eng)} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GEYw_91-ZZU",
        "outputId": "c57c36b6-5900-4478-d7be-853026e80511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 1406 samples\n",
            "Validation set: 301 samples\n",
            "Test set: 302 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training set english first 5 samples: {train_eng[:5]}\")\n",
        "print(f\"Training set urdu first 5 samples: {train_urdu[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRfM9dTvNBSQ",
        "outputId": "7e2057b4-0275-41cc-9271-3ea58b6f8e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set english first 5 samples: tensor([[   1,   43,   13,  310, 3859, 3865,  348, 3866, 3867,   37, 1601, 2050,\n",
            "         1302,   44,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1,  432,   17, 1613, 2908, 3230,  853, 3786,    6,  701,   58,   17,\n",
            "         3787, 3788, 3789,    6,   13,   17,  627, 3790, 3791,   25, 3792,    6,\n",
            "         3793, 2464,   58, 2843, 3794,    9, 2819, 2065,  612, 2172, 3795,   44,\n",
            "            2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1,  773, 1731,  505,    9, 5740, 5695,   60, 5716, 1964,   56, 5741,\n",
            "           21,    9, 2209, 1327, 5742,  580, 2141,   58, 5619, 5743,   44,    2,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1,  142, 1058, 6117,    9, 6118,  138, 3365,    6,    9, 6118, 6119,\n",
            "            9, 6116,  662,    9, 6120, 6121,   56, 1460,    9, 6107, 5435,   44,\n",
            "            2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1,  501, 3662,   17, 3983, 6805,  107,   17, 1942, 3713, 1494,   58,\n",
            "          732,   17,  553,  348, 2835,   60, 2192,   44,    2,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0]])\n",
            "Training set urdu first 5 samples: tensor([[   1,  261,   43,   30,  557,   13, 1525, 4329, 1927,  423, 4330, 1729,\n",
            "         2342, 1005,  434, 1783,   87,  575,   57,   88,    2,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1, 4234,    5, 3706,  285, 4235,    5, 2214,   35,  269, 2894, 1474,\n",
            "           22,   23,  562, 4236,  111,    5, 3191, 4237,    5, 1440,  550,  104,\n",
            "          279,   76,  559, 1145, 1476,    5, 3191, 4238,    5, 2410, 4239, 3776,\n",
            "           88,  775, 4240, 4241,   88,    2,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1,  897,   12, 2603,  671, 2255,   43, 6412, 3724,    5, 1027,   27,\n",
            "         2603, 6413,  175,   27,   71,  929,   21,   58, 6201,  545,   69,   25,\n",
            "          514,  898, 6414,   25, 6415,   27, 6416, 6417, 3724,  353, 6418,  577,\n",
            "         4669,  569,   88,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1, 2024,    5, 1088, 6799,   27,  126,   27, 1102,   49, 1710,    5,\n",
            "          151, 1088, 6799, 2511, 6800,   43, 6801, 6529,   22,   71, 6791, 6802,\n",
            "          104,  678,  322,   22,   88,    2,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1, 2466,   43, 6565,  423, 3181,   25, 6716,    5,  154, 2227,  192,\n",
            "         4904,    5, 7547, 7542,   27,  204, 1078,   22,   88,    2,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure split tensors are valid\n",
        "for split_name, split_data in zip(\n",
        "    [\"train_eng\", \"val_eng\", \"test_eng\", \"train_urdu\", \"val_urdu\", \"test_urdu\"],\n",
        "    [train_eng, val_eng, test_eng, train_urdu, val_urdu, test_urdu]\n",
        "):\n",
        "    assert torch.max(split_data) < len(english_vocab) if \"eng\" in split_name else len(urdu_vocab), \\\n",
        "        f\"{split_name} contains out-of-range indices!\"\n",
        "    assert torch.min(split_data) >= 0, f\"{split_name} contains negative indices!\"\n",
        "    print(f\"{split_name} indices are valid.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXCP40tavjXV",
        "outputId": "592d1cf6-b140-4d36-e6e0-0c37497e10c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_eng indices are valid.\n",
            "val_eng indices are valid.\n",
            "test_eng indices are valid.\n",
            "train_urdu indices are valid.\n",
            "val_urdu indices are valid.\n",
            "test_urdu indices are valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"English vocab size:\", len(english_vocab))\n",
        "print(\"Urdu vocab size:\", len(urdu_vocab))\n",
        "print(\"Max English token index:\", english_tensors.max().item())\n",
        "print(\"Max Urdu token index:\", urdu_tensors.max().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBAyNs5s8gFU",
        "outputId": "430ab700-6f2e-4767-a98b-310ea3fdbfec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocab size: 8651\n",
            "Urdu vocab size: 9555\n",
            "Max English token index: 8650\n",
            "Max Urdu token index: 9554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "urdu_embedding_layer = nn.Embedding.from_pretrained(torch.tensor(urdu_embeddings, dtype=torch.float32), freeze=False)\n",
        "assert urdu_embedding_layer.num_embeddings == len(urdu_vocab), \"Mismatch in vocabulary size and embedding layer.\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "8dPZS5ZZ87Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of embeddings: {english_embeddings.shape[0]}\")\n",
        "print(f\"Vocabulary size: {len(english_vocab)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3fpM0GJ9KCY",
        "outputId": "bd616487-f951-4fd7-fe92-0f8c7a9af204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embeddings: 8651\n",
            "Vocabulary size: 8651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Cell\n",
        "\n",
        "##Forward pass arguments:\n",
        "- x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
        "- h_prev (torch.Tensor): Previous hidden state of shape (batch_size, hidden_size).\n",
        "- c_prev (torch.Tensor): Previous cell state of shape (batch_size, hidden_size).\n",
        "\n",
        "## Forward pass outputs:      \n",
        "- h_t (torch.Tensor): Current hidden state.\n",
        "- c_t (torch.Tensor): Current cell state."
      ],
      "metadata": {
        "id": "V4sjNNNdK1gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    #constructor\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "\n",
        "        super(LSTMCell, self).__init__()\n",
        "        print(\"LSTM cell with \" + str(input_size) + \" input size and \" + str(hidden_size) + \" hidden size\")\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Weights for gates and candidate cell\n",
        "        self.W_forget = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.W_input = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.W_candidate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.W_output = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        # Concatenate input and hidden state\n",
        "        combined = torch.cat((x, h_prev), dim=1)  # Shape: (batch_size, input_size + hidden_size)\n",
        "\n",
        "        forget_gate = torch.sigmoid(self.W_forget(combined))\n",
        "        input_gate = torch.sigmoid(self.W_input(combined))\n",
        "        candidate_tilde = torch.tanh(self.W_candidate(combined))\n",
        "        cell_state = forget_gate * c_prev + input_gate * candidate_tilde\n",
        "        output_gate = torch.sigmoid(self.W_output(combined))\n",
        "        hidden_gate = output_gate * torch.tanh(cell_state)\n",
        "\n",
        "        # All have Shape: (batch_size, hidden_size)\n",
        "        return hidden_gate, cell_state"
      ],
      "metadata": {
        "id": "MVkZkfSqK0Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder of LSTM\n"
      ],
      "metadata": {
        "id": "Xy8V-XV6VW08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_layer, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding_layer\n",
        "        self.lstm_cell = LSTMCell(embedding_layer.embedding_dim, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length = x.size()\n",
        "        hidden = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "        cell = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "\n",
        "        all_hidden_states = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = self.embedding(x[:, t]) #word2vec embedding\n",
        "            hidden, cell = self.lstm_cell(x_t, hidden, cell)\n",
        "            all_hidden_states.append(hidden.unsqueeze(1))\n",
        "\n",
        "        all_hidden_states = torch.cat(all_hidden_states, dim=1)\n",
        "        return all_hidden_states, hidden, cell"
      ],
      "metadata": {
        "id": "9jWABQ7IRscG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention"
      ],
      "metadata": {
        "id": "QQD7fT4ERu0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: (batch_size, hidden_size)\n",
        "        # encoder_outputs: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        seq_length = encoder_outputs.size(1)\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, seq_length, 1)\n",
        "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention_weights = torch.softmax(self.v(energy).squeeze(2), dim=1)  # (batch_size, seq_length)\n",
        "\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, hidden_size)\n",
        "        return context.squeeze(1), attention_weights"
      ],
      "metadata": {
        "id": "qEadnx3pRq7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder for LSTM Cell\n"
      ],
      "metadata": {
        "id": "17aUv0WBY1pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_layer, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding_layer\n",
        "        self.attention = Attention(hidden_size)\n",
        "        self.lstm_cell = LSTMCell(embedding_layer.embedding_dim + hidden_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h, c, encoder_outputs):\n",
        "\n",
        "      x = self.embedding(x).squeeze(1)  # (batch_size, embedding_dim)\n",
        "      context, attention_weights = self.attention(h, encoder_outputs)\n",
        "      x_context = torch.cat((x, context), dim=1)\n",
        "      h, c = self.lstm_cell(x_context, h, c)\n",
        "      output = self.fc(h)  # (batch_size, output_size)\n",
        "      return output, h, c, attention_weights"
      ],
      "metadata": {
        "id": "aHeDRLMbWLRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq Model"
      ],
      "metadata": {
        "id": "uIEUn-ndZNAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        batch_size, tgt_seq_length = tgt.size()\n",
        "        outputs = torch.zeros(batch_size, tgt_seq_length, self.decoder.output_size).to(src.device)\n",
        "\n",
        "        decoder_input = tgt[:, 0].unsqueeze(1)  # Start with <SOS>\n",
        "        for t in range(1, tgt_seq_length):\n",
        "            output, hidden, cell, _ = self.decoder(decoder_input, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # Decide to use teacher forcing\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1).unsqueeze(1)  # Predicted word\n",
        "            decoder_input = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "lgN7MjycZGwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "cj8JPnZJaVWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "#Initialize embedding layers from word2vec pretrained\n",
        "english_embedding_layer = nn.Embedding.from_pretrained(torch.tensor(english_embeddings, dtype=torch.float32), freeze=False)\n",
        "urdu_embedding_layer = nn.Embedding.from_pretrained(torch.tensor(urdu_embeddings, dtype=torch.float32), freeze=False)\n",
        "\n",
        "hidden_size = 256\n",
        "BATCH_SIZE = 32\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(train_urdu, train_eng)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# urdu to english nmt\n",
        "encoder = Encoder(urdu_embedding_layer, hidden_size)\n",
        "decoder = Decoder(english_embedding_layer, hidden_size, len(english_vocab))\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "#Loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<PAD>\"]) #ignore <PAD>\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#print src and target from dataloader\n",
        "for src, tgt in dataloader:\n",
        "  print(f\"src shape: {src.shape}, max index: {torch.max(src)}, min index: {torch.min(src)}\")\n",
        "  print(f\"tgt shape: {tgt.shape}, max index: {torch.max(tgt)}, min index: {torch.min(tgt)}\")\n",
        "  break\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  for src, tgt in dataloader:\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(src, tgt)\n",
        "    output = outputs[:, 1:].reshape(-1, outputs.shape[2])\n",
        "    tgt = tgt[:, 1:].reshape(-1)\n",
        "    loss = criterion(output, tgt)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "  # Save the model checkpoint after each epoch\n",
        "  checkpoint_path = f\"seq2seq_checkpoint_epoch_{epoch}.pth\"\n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': total_loss / len(dataloader),\n",
        "      'english_vocab': english_vocab,\n",
        "      'urdu_vocab': urdu_vocab,\n",
        "      }, checkpoint_path)\n",
        "  print(f\"Checkpoint saved at {checkpoint_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x1k2RdY_dpiv",
        "outputId": "b7febc19-caee-4015-97e9-5d1e462eb880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "LSTM cell with 300 input size and 256 hidden size\n",
            "LSTM cell with 556 input size and 256 hidden size\n",
            "src shape: torch.Size([32, 50]), max index: 9221, min index: 0\n",
            "tgt shape: torch.Size([32, 50]), max index: 8339, min index: 0\n",
            "Epoch 1/10, Loss: 7.4263\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_0.pth\n",
            "Epoch 2/10, Loss: 6.7477\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_1.pth\n",
            "Epoch 3/10, Loss: 6.6428\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_2.pth\n",
            "Epoch 4/10, Loss: 6.5685\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_3.pth\n",
            "Epoch 5/10, Loss: 6.5080\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_4.pth\n",
            "Epoch 6/10, Loss: 6.4427\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_5.pth\n",
            "Epoch 7/10, Loss: 6.3678\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_6.pth\n",
            "Epoch 8/10, Loss: 6.3032\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_7.pth\n",
            "Epoch 9/10, Loss: 6.2243\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_8.pth\n",
            "Epoch 10/10, Loss: 6.1502\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_9.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "checkpoint_path = \"seq2seq_checkpoint_epoch_9.pth\"\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n",
        "\n",
        "# Restore the English and Urdu embedding layers\n",
        "english_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(english_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "urdu_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(urdu_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "\n",
        "hidden_size =256\n",
        "BATCH_SIZE=32\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(train_urdu, train_eng)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Restore the model\n",
        "encoder = Encoder(urdu_embedding_layer, hidden_size)\n",
        "decoder = Decoder(english_embedding_layer, hidden_size, len(checkpoint['english_vocab']))\n",
        "model = Seq2Seq(encoder, decoder).to(\"cuda\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Restore the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#Loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<PAD>\"]) #ignore <PAD>\n",
        "\n",
        "# Restore metadata\n",
        "start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
        "loss = checkpoint['loss']\n",
        "english_vocab = checkpoint['english_vocab']\n",
        "urdu_vocab = checkpoint['urdu_vocab']\n",
        "\n",
        "print(f\"Resuming training from epoch {start_epoch}, last loss: {loss:.4f}\")\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(\"cuda\"), tgt.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, tgt)\n",
        "        output = outputs[:, 1:].reshape(-1, outputs.shape[2])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # Save the updated checkpoint\n",
        "    checkpoint_path = f\"seq2seq_checkpoint_epoch_{epoch}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'english_vocab': english_vocab,\n",
        "        'urdu_vocab': urdu_vocab,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3HnFZi15Ow4b",
        "outputId": "c4b1d2f5-65fe-4385-e08c-8341bb9fbc06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-77de308042a4>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM cell with 300 input size and 256 hidden size\n",
            "LSTM cell with 556 input size and 256 hidden size\n",
            "Resuming training from epoch 10, last loss: 6.1502\n",
            "Epoch 11/50, Loss: 6.0968\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_10.pth\n",
            "Epoch 12/50, Loss: 6.0420\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_11.pth\n",
            "Epoch 13/50, Loss: 5.9839\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_12.pth\n",
            "Epoch 14/50, Loss: 5.9243\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_13.pth\n",
            "Epoch 15/50, Loss: 5.8734\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_14.pth\n",
            "Epoch 16/50, Loss: 5.7943\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_15.pth\n",
            "Epoch 17/50, Loss: 5.7581\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_16.pth\n",
            "Epoch 18/50, Loss: 5.6825\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_17.pth\n",
            "Epoch 19/50, Loss: 5.6369\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_18.pth\n",
            "Epoch 20/50, Loss: 5.5568\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_19.pth\n",
            "Epoch 21/50, Loss: 5.4984\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_20.pth\n",
            "Epoch 22/50, Loss: 5.4106\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_21.pth\n",
            "Epoch 23/50, Loss: 5.3722\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_22.pth\n",
            "Epoch 24/50, Loss: 5.3275\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_23.pth\n",
            "Epoch 25/50, Loss: 5.2706\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_24.pth\n",
            "Epoch 26/50, Loss: 5.2087\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_25.pth\n",
            "Epoch 27/50, Loss: 5.1345\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_26.pth\n",
            "Epoch 28/50, Loss: 5.0783\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_27.pth\n",
            "Epoch 29/50, Loss: 5.0143\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_28.pth\n",
            "Epoch 30/50, Loss: 4.9697\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_29.pth\n",
            "Epoch 31/50, Loss: 4.8886\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_30.pth\n",
            "Epoch 32/50, Loss: 4.8183\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_31.pth\n",
            "Epoch 33/50, Loss: 4.7911\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_32.pth\n",
            "Epoch 34/50, Loss: 4.7293\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_33.pth\n",
            "Epoch 35/50, Loss: 4.6685\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_34.pth\n",
            "Epoch 36/50, Loss: 4.6301\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_35.pth\n",
            "Epoch 37/50, Loss: 4.5480\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_36.pth\n",
            "Epoch 38/50, Loss: 4.4815\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_37.pth\n",
            "Epoch 39/50, Loss: 4.4758\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_38.pth\n",
            "Epoch 40/50, Loss: 4.4037\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_39.pth\n",
            "Epoch 41/50, Loss: 4.3131\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_40.pth\n",
            "Epoch 42/50, Loss: 4.2572\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_41.pth\n",
            "Epoch 43/50, Loss: 4.1717\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_42.pth\n",
            "Epoch 44/50, Loss: 4.1482\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_43.pth\n",
            "Epoch 45/50, Loss: 4.0619\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_44.pth\n",
            "Epoch 46/50, Loss: 4.0249\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_45.pth\n",
            "Epoch 47/50, Loss: 3.9636\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_46.pth\n",
            "Epoch 48/50, Loss: 3.8990\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_47.pth\n",
            "Epoch 49/50, Loss: 3.8089\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_48.pth\n",
            "Epoch 50/50, Loss: 3.7250\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_49.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "checkpoint_path = \"seq2seq_checkpoint_epoch_49.pth\"\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n",
        "\n",
        "# Restore the English and Urdu embedding layers\n",
        "english_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(english_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "urdu_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(urdu_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "\n",
        "hidden_size =256\n",
        "BATCH_SIZE=32\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(train_urdu, train_eng)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Restore the model\n",
        "encoder = Encoder(urdu_embedding_layer, hidden_size)\n",
        "decoder = Decoder(english_embedding_layer, hidden_size, len(checkpoint['english_vocab']))\n",
        "model = Seq2Seq(encoder, decoder).to(\"cuda\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Restore the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#Loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<PAD>\"]) #ignore <PAD>\n",
        "\n",
        "# Restore metadata\n",
        "start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
        "loss = checkpoint['loss']\n",
        "english_vocab = checkpoint['english_vocab']\n",
        "urdu_vocab = checkpoint['urdu_vocab']\n",
        "\n",
        "print(f\"Resuming training from epoch {start_epoch}, last loss: {loss:.4f}\")\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(\"cuda\"), tgt.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, tgt)\n",
        "        output = outputs[:, 1:].reshape(-1, outputs.shape[2])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # Save the updated checkpoint\n",
        "    checkpoint_path = f\"seq2seq_checkpoint_epoch_{epoch}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'english_vocab': english_vocab,\n",
        "        'urdu_vocab': urdu_vocab,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVLdMQhUFWd",
        "outputId": "73267725-ce51-4511-d336-7f5c1da7e23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-704b48d3405b>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM cell with 300 input size and 256 hidden size\n",
            "LSTM cell with 556 input size and 256 hidden size\n",
            "Resuming training from epoch 50, last loss: 3.7250\n",
            "Epoch 51/100, Loss: 3.6853\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_50.pth\n",
            "Epoch 52/100, Loss: 3.5971\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_51.pth\n",
            "Epoch 53/100, Loss: 3.5398\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_52.pth\n",
            "Epoch 54/100, Loss: 3.4565\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_53.pth\n",
            "Epoch 55/100, Loss: 3.3994\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_54.pth\n",
            "Epoch 56/100, Loss: 3.3315\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_55.pth\n",
            "Epoch 57/100, Loss: 3.2016\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_56.pth\n",
            "Epoch 58/100, Loss: 3.1514\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_57.pth\n",
            "Epoch 59/100, Loss: 3.0642\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_58.pth\n",
            "Epoch 60/100, Loss: 3.0155\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_59.pth\n",
            "Epoch 61/100, Loss: 2.9711\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_60.pth\n",
            "Epoch 62/100, Loss: 2.9404\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_61.pth\n",
            "Epoch 63/100, Loss: 2.7990\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_62.pth\n",
            "Epoch 64/100, Loss: 2.7151\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_63.pth\n",
            "Epoch 65/100, Loss: 2.6822\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_64.pth\n",
            "Epoch 66/100, Loss: 2.5600\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_65.pth\n",
            "Epoch 67/100, Loss: 2.5592\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_66.pth\n",
            "Epoch 68/100, Loss: 2.4561\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_67.pth\n",
            "Epoch 69/100, Loss: 2.3664\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_68.pth\n",
            "Epoch 70/100, Loss: 2.3510\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_69.pth\n",
            "Epoch 71/100, Loss: 2.2733\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_70.pth\n",
            "Epoch 72/100, Loss: 2.2434\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_71.pth\n",
            "Epoch 73/100, Loss: 2.1428\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_72.pth\n",
            "Epoch 74/100, Loss: 2.0986\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_73.pth\n",
            "Epoch 75/100, Loss: 2.0456\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_74.pth\n",
            "Epoch 76/100, Loss: 1.9455\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_75.pth\n",
            "Epoch 77/100, Loss: 1.9200\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_76.pth\n",
            "Epoch 78/100, Loss: 1.8551\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_77.pth\n",
            "Epoch 79/100, Loss: 1.8080\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_78.pth\n",
            "Epoch 80/100, Loss: 1.7333\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_79.pth\n",
            "Epoch 81/100, Loss: 1.6813\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_80.pth\n",
            "Epoch 82/100, Loss: 1.6359\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_81.pth\n",
            "Epoch 83/100, Loss: 1.5969\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_82.pth\n",
            "Epoch 84/100, Loss: 1.5273\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_83.pth\n",
            "Epoch 85/100, Loss: 1.4727\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_84.pth\n",
            "Epoch 86/100, Loss: 1.4497\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_85.pth\n",
            "Epoch 87/100, Loss: 1.4000\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_86.pth\n",
            "Epoch 88/100, Loss: 1.3430\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_87.pth\n",
            "Epoch 89/100, Loss: 1.2853\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_88.pth\n",
            "Epoch 90/100, Loss: 1.2704\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_89.pth\n",
            "Epoch 91/100, Loss: 1.1991\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_90.pth\n",
            "Epoch 92/100, Loss: 1.1319\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_91.pth\n",
            "Epoch 93/100, Loss: 1.0986\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_92.pth\n",
            "Epoch 94/100, Loss: 1.0575\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_93.pth\n",
            "Epoch 95/100, Loss: 1.0213\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_94.pth\n",
            "Epoch 96/100, Loss: 0.9873\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_95.pth\n",
            "Epoch 97/100, Loss: 0.9307\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_96.pth\n",
            "Epoch 98/100, Loss: 0.9111\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_97.pth\n",
            "Epoch 99/100, Loss: 0.8687\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_98.pth\n",
            "Epoch 100/100, Loss: 0.8174\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_99.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "checkpoint_path = \"seq2seq_checkpoint_epoch_99.pth\"\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n",
        "\n",
        "# Restore the English and Urdu embedding layers\n",
        "english_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(english_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "urdu_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(urdu_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "\n",
        "hidden_size =256\n",
        "BATCH_SIZE=32\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(train_urdu, train_eng)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Restore the model\n",
        "encoder = Encoder(urdu_embedding_layer, hidden_size)\n",
        "decoder = Decoder(english_embedding_layer, hidden_size, len(checkpoint['english_vocab']))\n",
        "model = Seq2Seq(encoder, decoder).to(\"cuda\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Restore the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#Loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<PAD>\"]) #ignore <PAD>\n",
        "\n",
        "# Restore metadata\n",
        "start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
        "loss = checkpoint['loss']\n",
        "english_vocab = checkpoint['english_vocab']\n",
        "urdu_vocab = checkpoint['urdu_vocab']\n",
        "\n",
        "print(f\"Resuming training from epoch {start_epoch}, last loss: {loss:.4f}\")\n",
        "\n",
        "num_epochs = 120\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(\"cuda\"), tgt.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, tgt)\n",
        "        output = outputs[:, 1:].reshape(-1, outputs.shape[2])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # Save the updated checkpoint\n",
        "    checkpoint_path = f\"seq2seq_checkpoint_epoch_{epoch}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'english_vocab': english_vocab,\n",
        "        'urdu_vocab': urdu_vocab,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3SoSn2J5WTcP",
        "outputId": "cf729afe-698a-4ea9-e7e4-f54f432c31eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-6e6ee30d818f>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM cell with 300 input size and 256 hidden size\n",
            "LSTM cell with 556 input size and 256 hidden size\n",
            "Resuming training from epoch 100, last loss: 0.8174\n",
            "Epoch 101/120, Loss: 0.7805\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_100.pth\n",
            "Epoch 102/120, Loss: 0.7559\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_101.pth\n",
            "Epoch 103/120, Loss: 0.7143\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_102.pth\n",
            "Epoch 104/120, Loss: 0.6991\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_103.pth\n",
            "Epoch 105/120, Loss: 0.6660\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_104.pth\n",
            "Epoch 106/120, Loss: 0.6395\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_105.pth\n",
            "Epoch 107/120, Loss: 0.6012\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_106.pth\n",
            "Epoch 108/120, Loss: 0.5738\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_107.pth\n",
            "Epoch 109/120, Loss: 0.5494\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_108.pth\n",
            "Epoch 110/120, Loss: 0.5290\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_109.pth\n",
            "Epoch 111/120, Loss: 0.5097\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_110.pth\n",
            "Epoch 112/120, Loss: 0.4617\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_111.pth\n",
            "Epoch 113/120, Loss: 0.4408\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_112.pth\n",
            "Epoch 114/120, Loss: 0.4265\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_113.pth\n",
            "Epoch 115/120, Loss: 0.4143\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_114.pth\n",
            "Epoch 116/120, Loss: 0.3793\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_115.pth\n",
            "Epoch 117/120, Loss: 0.3621\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_116.pth\n",
            "Epoch 118/120, Loss: 0.3351\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_117.pth\n",
            "Epoch 119/120, Loss: 0.3081\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_118.pth\n",
            "Epoch 120/120, Loss: 0.2914\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_119.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "checkpoint_path = \"seq2seq_checkpoint_epoch_119.pth\"\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n",
        "\n",
        "# Restore the English and Urdu embedding layers\n",
        "english_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(english_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "urdu_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(urdu_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "\n",
        "hidden_size =256\n",
        "BATCH_SIZE=32\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(train_urdu, train_eng)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Restore the model\n",
        "encoder = Encoder(urdu_embedding_layer, hidden_size)\n",
        "decoder = Decoder(english_embedding_layer, hidden_size, len(checkpoint['english_vocab']))\n",
        "model = Seq2Seq(encoder, decoder).to(\"cuda\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Restore the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#Loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<PAD>\"]) #ignore <PAD>\n",
        "\n",
        "# Restore metadata\n",
        "start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
        "loss = checkpoint['loss']\n",
        "english_vocab = checkpoint['english_vocab']\n",
        "urdu_vocab = checkpoint['urdu_vocab']\n",
        "\n",
        "print(f\"Resuming training from epoch {start_epoch}, last loss: {loss:.4f}\")\n",
        "\n",
        "num_epochs = 150\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(\"cuda\"), tgt.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, tgt)\n",
        "        output = outputs[:, 1:].reshape(-1, outputs.shape[2])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # Save the updated checkpoint\n",
        "    checkpoint_path = f\"seq2seq_checkpoint_epoch_{epoch}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'english_vocab': english_vocab,\n",
        "        'urdu_vocab': urdu_vocab,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u_VJerhXi9b",
        "outputId": "f0b7a11a-0511-4eaa-a44f-5f957318aac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-2653379d5d75>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM cell with 300 input size and 256 hidden size\n",
            "LSTM cell with 556 input size and 256 hidden size\n",
            "Resuming training from epoch 120, last loss: 0.2914\n",
            "Epoch 121/150, Loss: 0.2828\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_120.pth\n",
            "Epoch 122/150, Loss: 0.2635\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_121.pth\n",
            "Epoch 123/150, Loss: 0.2555\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_122.pth\n",
            "Epoch 124/150, Loss: 0.2378\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_123.pth\n",
            "Epoch 125/150, Loss: 0.2245\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_124.pth\n",
            "Epoch 126/150, Loss: 0.2089\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_125.pth\n",
            "Epoch 127/150, Loss: 0.2036\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_126.pth\n",
            "Epoch 128/150, Loss: 0.1881\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_127.pth\n",
            "Epoch 129/150, Loss: 0.1876\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_128.pth\n",
            "Epoch 130/150, Loss: 0.1724\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_129.pth\n",
            "Epoch 131/150, Loss: 0.1649\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_130.pth\n",
            "Epoch 132/150, Loss: 0.1519\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_131.pth\n",
            "Epoch 133/150, Loss: 0.1413\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_132.pth\n",
            "Epoch 134/150, Loss: 0.1308\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_133.pth\n",
            "Epoch 135/150, Loss: 0.1256\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_134.pth\n",
            "Epoch 136/150, Loss: 0.1224\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_135.pth\n",
            "Epoch 137/150, Loss: 0.1171\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_136.pth\n",
            "Epoch 138/150, Loss: 0.1152\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_137.pth\n",
            "Epoch 139/150, Loss: 0.1146\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_138.pth\n",
            "Epoch 140/150, Loss: 0.1046\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_139.pth\n",
            "Epoch 141/150, Loss: 0.0959\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_140.pth\n",
            "Epoch 142/150, Loss: 0.0915\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_141.pth\n",
            "Epoch 143/150, Loss: 0.0844\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_142.pth\n",
            "Epoch 144/150, Loss: 0.0782\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_143.pth\n",
            "Epoch 145/150, Loss: 0.0733\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_144.pth\n",
            "Epoch 146/150, Loss: 0.0697\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_145.pth\n",
            "Epoch 147/150, Loss: 0.0658\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_146.pth\n",
            "Epoch 148/150, Loss: 0.0627\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_147.pth\n",
            "Epoch 149/150, Loss: 0.0593\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_148.pth\n",
            "Epoch 150/150, Loss: 0.0569\n",
            "Checkpoint saved at seq2seq_checkpoint_epoch_149.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "eW1kcmFCOWuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_tokenize(sentence, vocab, max_length=50):\n",
        "    # Tokenize the sentence\n",
        "    tokens = sentence.split()\n",
        "    token_indices = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "\n",
        "    # Add <SOS> and <EOS> tokens, and pad\n",
        "    token_indices = [vocab['<SOS>']] + token_indices[:max_length - 2] + [vocab['<EOS>']]\n",
        "    token_indices += [vocab['<PAD>']] * (max_length - len(token_indices))\n",
        "\n",
        "    # Convert to tensor\n",
        "    return torch.tensor(token_indices, dtype=torch.long).unsqueeze(0)  # Shape: (1, seq_length)\n",
        "\n",
        "def translate(model, input_tensor, urdu_vocab, english_vocab, max_length=50):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Move input tensor to the same device as the model\n",
        "        input_tensor = input_tensor.to(next(model.parameters()).device)\n",
        "\n",
        "        # Encode the source sentence\n",
        "        encoder_outputs, encoder_hidden, encoder_cell = model.encoder(input_tensor)\n",
        "\n",
        "        # Initialize the decoder\n",
        "        decoder_input = torch.tensor([english_vocab['<SOS>']], device=input_tensor.device)\n",
        "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
        "\n",
        "        # Store translated tokens\n",
        "        translated_tokens = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Decode with attention\n",
        "            decoder_output, decoder_hidden, decoder_cell, attention_weights = model.decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # Get the token with the highest probability\n",
        "            top1 = decoder_output.argmax(1).item()\n",
        "\n",
        "            # Stop translation if <EOS> is predicted\n",
        "            if top1 == english_vocab['<EOS>']:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token\n",
        "            translated_tokens.append(top1)\n",
        "\n",
        "            # Set the next input to the predicted token\n",
        "            decoder_input = torch.tensor([top1], device=input_tensor.device)\n",
        "\n",
        "        # Map token indices back to words\n",
        "        idx_to_word = {idx: word for word, idx in english_vocab.items()}\n",
        "        translated_sentence = \" \".join([idx_to_word.get(idx, '<UNK>') for idx in translated_tokens])\n",
        "\n",
        "        return translated_sentence\n",
        "\n",
        "urdu_sentence = \"پیر کے روز، سٹینفورڈ اسکول آف میڈیسن کے سائنسدانوں نے ایک جدید تشخیصی آلہ دریافت کرنے کا اعلان کیا۔\"\n",
        "\n",
        "# Preprocess and tokenize the Urdu sentence\n",
        "input_tensor = preprocess_and_tokenize(urdu_sentence, urdu_vocab)\n",
        "\n",
        "# Perform translation\n",
        "translation = translate(model, input_tensor, urdu_vocab, english_vocab)\n",
        "\n",
        "print(\"Original Urdu:\", urdu_sentence)\n",
        "print(\"Translated English:\", translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tokiXQFQLH_q",
        "outputId": "6f5e4a5e-e498-4747-943f-c53d2d603cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Urdu: پیر کے روز، سٹینفورڈ اسکول آف میڈیسن کے سائنسدانوں نے ایک جدید تشخیصی آلہ دریافت کرنے کا اعلان کیا۔\n",
            "Translated English: in late over time , protesters to the major copyright holders ' official bomb open .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download last checkpoint\n",
        "from google.colab import files\n",
        "files.download('/content/seq2seq_checkpoint_epoch_149.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "37hVlYXE5hvv",
        "outputId": "ae9c456f-b237-4e9d-b17e-ba92b80bd793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8967a0d4-c8db-4e00-b550-c2dc791a4fd0\", \"seq2seq_checkpoint_epoch_149.pth\", 111058082)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading model for inference"
      ],
      "metadata": {
        "id": "WM8Tc9mom0UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# File path to the checkpoint\n",
        "checkpoint_path = \"/content/seq2seq_checkpoint_epoch_149 (1).pth\"\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, \"cpu\")\n",
        "\n",
        "# Restore the English and Urdu embedding layers\n",
        "english_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(english_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "urdu_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(urdu_embeddings, dtype=torch.float32), freeze=False\n",
        ")\n",
        "\n",
        "# Define model architecture parameters\n",
        "hidden_size = 256\n",
        "\n",
        "# Reconstruct the model\n",
        "encoder = Encoder(urdu_embedding_layer, hidden_size)\n",
        "decoder = Decoder(english_embedding_layer, hidden_size, len(checkpoint['english_vocab']))\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "# Load the model's state dict\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded and ready for inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1twZv5zfmv-A",
        "outputId": "bbc49b29-b5be-4bfa-a294-4f73a1be6f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-162-6100d7708314>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, \"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM cell with 300 input size and 256 hidden size\n",
            "LSTM cell with 556 input size and 256 hidden size\n",
            "Model loaded and ready for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bleu Score Evaluation"
      ],
      "metadata": {
        "id": "0ddaSUaVC8_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install perl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXvgJMehkw-o",
        "outputId": "1c9347a5-6ef8-4792-89c7-671320ba28ac",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libperl5.34 perl-base perl-modules-5.34\n",
            "Suggested packages:\n",
            "  perl-doc libterm-readline-gnu-perl | libterm-readline-perl-perl libtap-harness-archive-perl\n",
            "Recommended packages:\n",
            "  netbase\n",
            "The following packages will be upgraded:\n",
            "  libperl5.34 perl perl-base perl-modules-5.34\n",
            "4 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 9,790 kB of archives.\n",
            "After this operation, 8,192 B of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libperl5.34 amd64 5.34.0-3ubuntu1.3 [4,820 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl amd64 5.34.0-3ubuntu1.3 [232 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-base amd64 5.34.0-3ubuntu1.3 [1,762 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-modules-5.34 all 5.34.0-3ubuntu1.3 [2,976 kB]\n",
            "Fetched 9,790 kB in 1s (9,017 kB/s)\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../libperl5.34_5.34.0-3ubuntu1.3_amd64.deb ...\n",
            "Unpacking libperl5.34:amd64 (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n",
            "Preparing to unpack .../perl_5.34.0-3ubuntu1.3_amd64.deb ...\n",
            "Unpacking perl (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n",
            "Preparing to unpack .../perl-base_5.34.0-3ubuntu1.3_amd64.deb ...\n",
            "Unpacking perl-base (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n",
            "Setting up perl-base (5.34.0-3ubuntu1.3) ...\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../perl-modules-5.34_5.34.0-3ubuntu1.3_all.deb ...\n",
            "Unpacking perl-modules-5.34 (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n",
            "Setting up perl-modules-5.34 (5.34.0-3ubuntu1.3) ...\n",
            "Setting up libperl5.34:amd64 (5.34.0-3ubuntu1.3) ...\n",
            "Setting up perl (5.34.0-3ubuntu1.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_file = \"predictions.txt\"\n",
        "references_file = \"reference.txt\"\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Map indices to words for the English vocabulary\n",
        "idx_to_word_eng = {idx: word for word, idx in english_vocab.items()}\n",
        "\n",
        "# Open files for writing\n",
        "with open(predictions_file, \"w\") as pred_file, open(references_file, \"w\") as ref_file:\n",
        "    for i in range(len(test_urdu)):\n",
        "        # Prepare input tensor for the test Urdu sentence\n",
        "        input_tensor = test_urdu[i].unsqueeze(0).to(next(model.parameters()).device)\n",
        "        reference_tensor = test_eng[i]\n",
        "\n",
        "        # Generate translation using the updated translation function\n",
        "        predicted_translation = translate(model, input_tensor, urdu_vocab, english_vocab)\n",
        "\n",
        "        # Convert the reference tensor to words\n",
        "        reference_translation = \" \".join([\n",
        "            idx_to_word_eng.get(idx, \"<UNK>\")\n",
        "            for idx in reference_tensor.tolist()\n",
        "            if idx not in [english_vocab[\"<PAD>\"], english_vocab[\"<SOS>\"], english_vocab[\"<EOS>\"]]\n",
        "        ])\n",
        "\n",
        "        # Write predicted and reference translations to respective files\n",
        "        pred_file.write(predicted_translation + \"\\n\")\n",
        "        ref_file.write(reference_translation + \"\\n\")\n",
        "\n",
        "print(f\"Predictions saved to {predictions_file}\")\n",
        "print(f\"References saved to {references_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5qy0bFgaNTZ",
        "outputId": "0f5341a7-4bde-42c3-90e2-f5474dac8460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to predictions.txt\n",
            "References saved to reference.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!perl multi-bleu.perl reference.txt < predictions.txt"
      ],
      "metadata": {
        "id": "MgRsstEqHs74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87c6428-03f2-4e3d-a038-388a7779ed30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU = 0.45, 21.2/1.4/0.1/0.0 (BP=0.950, ratio=0.951, hyp_len=7039, ref_len=7399)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Try with scrableu"
      ],
      "metadata": {
        "id": "pVasjfy4k3yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu\n",
        "import sacrebleu\n",
        "\n",
        "# Read the reference file (each line corresponds to one reference translation)\n",
        "with open(\"reference.txt\", \"r\", encoding=\"utf-8\") as ref_file:\n",
        "    references = [line.strip() for line in ref_file]\n",
        "\n",
        "# Read the predictions file (each line corresponds to one predicted translation)\n",
        "with open(\"predictions.txt\", \"r\", encoding=\"utf-8\") as pred_file:\n",
        "    predictions = [line.strip() for line in pred_file]\n",
        "\n",
        "# Make sure the number of predictions matches the number of references\n",
        "assert len(predictions) == len(references), \"Mismatch in number of predictions and references!\"\n",
        "\n",
        "# SacreBLEU requires references to be a list of lists (one list per reference set)\n",
        "references = [references]  # Wrap references in another list for SacreBLEU\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu = sacrebleu.corpus_bleu(predictions, references)\n",
        "\n",
        "print(f\"SacreBLEU score: {bleu.score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahXAAePKcL_S",
        "outputId": "c8e00a53-0b74-43ee-e3b2-44064831cb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (3.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SacreBLEU score: 0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "MF8G6RjFprnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Training Loss:** 0.0569 (at Epoch 150)\n",
        "- **Validation Loss:** 12.2101\n",
        "- **Validation BLEU Score:** 0.36\n",
        "- **Test Loss:** 12.1289\n",
        "- **Test BLEU Score:** 0.46\n"
      ],
      "metadata": {
        "id": "2kqLNw5Cu1sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|       Metric      | Training Set | Validation Set | Test Set |\n",
        "|--------------------|--------------|----------------|----------|\n",
        "| Loss              | 0.0569       | 12.2046        | 12.0811  |\n",
        "| BLEU Score        | 0.45          | 0.3600         | 0.4600   |\n"
      ],
      "metadata": {
        "id": "yqe9O5p1wPyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_loss = 0.0\n",
        "val_predictions = []\n",
        "val_references = []\n",
        "\n",
        "hidden_size =256\n",
        "BATCH_SIZE=32\n",
        "# Create DataLoader for validation set\n",
        "dataset = TensorDataset(val_urdu, val_eng)\n",
        "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "for src, tgt in val_loader:\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(src, tgt)\n",
        "    output = outputs[:, 1:].reshape(-1, outputs.shape[2])  # Flattened predictions\n",
        "    reshaped_tgt = tgt[:, 1:].reshape(-1)  # Flattened targets\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(output, reshaped_tgt)\n",
        "    val_loss += loss.item()\n",
        "    #print loss\n",
        "    print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
        "\n",
        "    # Store predictions and references for BLEU\n",
        "    for i in range(src.size(0)):\n",
        "        # Predicted translation\n",
        "        predicted_translation = translate(\n",
        "            model, src[i].unsqueeze(0).to(next(model.parameters()).device), urdu_vocab, english_vocab\n",
        "        )\n",
        "        val_predictions.append(predicted_translation)\n",
        "\n",
        "        reference_translation = \" \".join([\n",
        "            idx_to_word_eng.get(idx, \"<UNK>\")\n",
        "            for idx in tgt[i, 1:].tolist()\n",
        "            if idx not in [english_vocab[\"<PAD>\"], english_vocab[\"<SOS>\"], english_vocab[\"<EOS>\"]]\n",
        "        ])\n",
        "        val_references.append(reference_translation)\n",
        "\n",
        "# Compute BLEU score\n",
        "val_bleu = sacrebleu.corpus_bleu(val_predictions, [val_references])\n",
        "print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
        "print(f\"Validation BLEU Score: {val_bleu.score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMhH2GCMoXu_",
        "outputId": "77fba804-36a8-435c-9fb1-7d97f4639e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.2197\n",
            "Validation Loss: 2.4558\n",
            "Validation Loss: 3.6753\n",
            "Validation Loss: 4.8987\n",
            "Validation Loss: 6.0703\n",
            "Validation Loss: 7.2800\n",
            "Validation Loss: 8.4964\n",
            "Validation Loss: 9.7649\n",
            "Validation Loss: 10.9703\n",
            "Validation Loss: 12.1458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 12.1458\n",
            "Validation BLEU Score: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_loss = 0.0\n",
        "val_predictions = []\n",
        "val_references = []\n",
        "\n",
        "hidden_size =256\n",
        "BATCH_SIZE=32\n",
        "# Create DataLoader for test set\n",
        "dataset = TensorDataset(test_urdu, test_eng)\n",
        "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "for src, tgt in val_loader:\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(src, tgt)\n",
        "    output = outputs[:, 1:].reshape(-1, outputs.shape[2])  # Flattened predictions\n",
        "    reshaped_tgt = tgt[:, 1:].reshape(-1)  # Flattened targets\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(output, reshaped_tgt)\n",
        "    val_loss += loss.item()\n",
        "    print(f\"Test Loss: {val_loss / len(val_loader):.4f}\")\n",
        "\n",
        "    # Store predictions and references for BLEU\n",
        "    for i in range(src.size(0)):\n",
        "        # Predicted translation\n",
        "        predicted_translation = translate(\n",
        "            model, src[i].unsqueeze(0), urdu_vocab, english_vocab\n",
        "        )\n",
        "        val_predictions.append(predicted_translation)\n",
        "\n",
        "        reference_translation = \" \".join([\n",
        "            idx_to_word_eng.get(idx, \"<UNK>\")\n",
        "            for idx in tgt[i, 1:].tolist()\n",
        "            if idx not in [english_vocab[\"<PAD>\"], english_vocab[\"<SOS>\"], english_vocab[\"<EOS>\"]]\n",
        "        ])\n",
        "        val_references.append(reference_translation)\n",
        "\n",
        "# Compute BLEU score\n",
        "val_bleu = sacrebleu.corpus_bleu(val_predictions, [val_references])\n",
        "print(f\"Test Loss: {val_loss / len(val_loader):.4f}\")\n",
        "print(f\"Test BLEU Score: {val_bleu.score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7eYPF0mq5b1",
        "outputId": "4917f5e6-e819-4bc3-d88b-5c08f1bb84dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.2579\n",
            "Test Loss: 2.4846\n",
            "Test Loss: 3.6419\n",
            "Test Loss: 4.8117\n",
            "Test Loss: 6.0655\n",
            "Test Loss: 7.3339\n",
            "Test Loss: 8.5256\n",
            "Test Loss: 9.7349\n",
            "Test Loss: 10.9922\n",
            "Test Loss: 12.1950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 12.1950\n",
            "Test BLEU Score: 0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "!pip install ace_tools\n",
        "\n",
        "# Data for training, validation, and test sets\n",
        "epochs = list(range(125, 151))\n",
        "train_losses = [\n",
        "    0.2245, 0.2089, 0.2036, 0.1881, 0.1876, 0.1724, 0.1649, 0.1519, 0.1413,\n",
        "    0.1308, 0.1256, 0.1224, 0.1171, 0.1152, 0.1146, 0.1046, 0.0959, 0.0915,\n",
        "    0.0844, 0.0782, 0.0733, 0.0697, 0.0658, 0.0627, 0.0593, 0.0569\n",
        "]\n",
        "\n",
        "val_losses = [\n",
        "    1.2414, 2.4494, 3.7455, 4.9628, 6.1793, 7.4079, 8.6252, 9.8199, 10.9711,\n",
        "    12.2046, 12.2046, 12.2046, 12.2046, 12.2046, 12.2046, 12.2046, 12.2046,\n",
        "    12.2046, 12.2046, 12.2046, 12.2046, 12.2046, 12.2046, 12.2046, 12.2046, 12.2046\n",
        "]\n",
        "\n",
        "test_losses = [\n",
        "    1.2062, 2.3623, 3.5896, 4.7601, 5.9464, 7.1562, 8.4032, 9.6444, 10.8664,\n",
        "    12.0811, 12.0811, 12.0811, 12.0811, 12.0811, 12.0811, 12.0811, 12.0811,\n",
        "    12.0811, 12.0811, 12.0811, 12.0811, 12.0811, 12.0811, 12.0811, 12.0811, 12.0811\n",
        "]\n",
        "\n",
        "val_bleu_score = 0.36\n",
        "test_bleu_score = 0.46\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_losses, label='Training Loss')\n",
        "plt.plot(epochs, val_losses[:len(epochs)], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, test_losses[:len(epochs)], label='Test Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Results table\n",
        "data = {\n",
        "    \"Metric\": [\"Loss\", \"BLEU Score\"],\n",
        "    \"Training Set\": [train_losses[-1], \"N/A\"],\n",
        "    \"Validation Set\": [val_losses[-1], val_bleu_score],\n",
        "    \"Test Set\": [test_losses[-1], test_bleu_score],\n",
        "}\n",
        "results_df = pd.DataFrame(data)\n",
        "print(\"Results Table:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aZy1SqsisL4D",
        "outputId": "06041d09-2f15-4658-bee0-b4ff52c79aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ace_tools in /usr/local/lib/python3.10/dist-packages (0.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results Table:\n",
            "       Metric Training Set  Validation Set  Test Set\n",
            "0        Loss       0.0569         12.2046   12.0811\n",
            "1  BLEU Score          N/A          0.3600    0.4600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQZR0yW-wQWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}